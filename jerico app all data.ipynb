{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\chtn\\gen_ai\\hitesh\\project_xa001\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def get_data_from_website(url):\n",
    "    # Get response from the server\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 500:\n",
    "        print(\"Server error\")\n",
    "        return\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 1: Find all tab titles\n",
    "    tab_titles = soup.find_all(\"div\", class_=\"elementor-tab-title\")\n",
    "\n",
    "    # Step 2: Find all corresponding tab contents\n",
    "    tab_data = {}\n",
    "\n",
    "    for title_div in tab_titles:\n",
    "        tab_id = title_div.get(\"data-tab\")\n",
    "        tab_title = title_div.get_text(strip=True)\n",
    "\n",
    "        matching_content = soup.find(\"div\", class_=\"elementor-tab-content\", attrs={\"data-tab\": tab_id})\n",
    "        tab_content = matching_content.get_text(separator=\"\\n\", strip=True) if matching_content else \"\"\n",
    "\n",
    "        tab_data[tab_title] = tab_content\n",
    "\n",
    "    # âœ… Create 'data' folder if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    # âœ… Save as JSON file\n",
    "    with open(\"data/tab_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tab_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Data saved to data/tab_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data/tab_data.json\n"
     ]
    }
   ],
   "source": [
    "get_data_from_website(\"https://www.dinecollege.edu/academics/academic-policies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ferpa_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from a FERPA-related website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    data = {}\n",
    "    h3_tags = soup.find_all('h3')\n",
    "    for h3_tag in h3_tags:\n",
    "        question = h3_tag.text.strip()\n",
    "        answer_parts = []\n",
    "        sibling = h3_tag.find_next_sibling()\n",
    "        while sibling and sibling.name in ['p', 'div', 'ul', 'ol']:\n",
    "            answer_parts.append(sibling.text.strip())\n",
    "            sibling = sibling.find_next_sibling()\n",
    "        answer = \" \".join(answer_parts).strip()\n",
    "        if question and answer:\n",
    "            data[question] = answer\n",
    "\n",
    "    modified_data = {}\n",
    "    for key, value in data.items():\n",
    "        new_key = key.rstrip(\":\")\n",
    "        modified_value = value.replace(\"Back to Top\", \"\").strip()\n",
    "        if len(modified_value.split()) >= 5:\n",
    "            modified_data[new_key] = modified_value\n",
    "\n",
    "    _append_to_json(modified_data, output_filename)\n",
    "\n",
    "def append_civil_rights_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from a civil rights laws website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    final_data = {}\n",
    "\n",
    "    # Extract the main heading and description\n",
    "    main_heading = soup.find('h1', class_='usa-hero__heading')\n",
    "    main_desc = soup.find('div', class_='field--name-body')\n",
    "\n",
    "    if main_heading and main_desc:\n",
    "        title = main_heading.text.strip()\n",
    "        description = main_desc.text.strip()\n",
    "        final_data[title] = description\n",
    "\n",
    "    # Now extract the cards information\n",
    "    cards = soup.find_all('div', class_='card-image-top-txt')\n",
    "\n",
    "    for card in cards:\n",
    "        card_title_tag = card.find('div', class_='field--name-field-ed-card-image-top-title')\n",
    "        card_summary_tag = card.find('div', class_='field--name-field-ed-card-image-top-summary')\n",
    "        card_link_tag = card.find('div', class_='field--name-field-ed-card-image-top-link')\n",
    "\n",
    "        if card_title_tag and card_summary_tag:\n",
    "            card_title = card_title_tag.text.strip()\n",
    "            card_summary = card_summary_tag.text.strip()\n",
    "\n",
    "            # Get the link if available\n",
    "            link = \"\"\n",
    "            if card_link_tag and card_link_tag.find('a'):\n",
    "                href = card_link_tag.find('a')['href']\n",
    "                if href.startswith(\"/\"):\n",
    "                    href = \"https://www.ed.gov\" + href\n",
    "                link = href\n",
    "            final_data[card_title] = f\"{card_summary} link :- {link}\".strip()\n",
    "\n",
    "    _append_to_json(final_data, output_filename)\n",
    "\n",
    "def append_file_complaint_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from the file a complaint website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove unnecessary tags\n",
    "    for tag in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove known banners or sections\n",
    "    for div in soup.find_all(['div', 'section'], class_=[\n",
    "        'usa-banner', 'header', 'navigation', 'menu', 'site-header',\n",
    "        'usa-footer', 'main-header', 'branding', 'footer-links'\n",
    "    ]):\n",
    "        div.decompose()\n",
    "\n",
    "    for elem in soup.find_all(id=[\n",
    "        'header', 'footer', 'navbar', 'skip-link', 'back-to-top'\n",
    "    ]):\n",
    "        elem.decompose()\n",
    "\n",
    "    # Get heading\n",
    "    heading = soup.find('h1')\n",
    "    key = heading.get_text(strip=True) if heading else \"No Heading Found\"\n",
    "\n",
    "    # Collect all visible text\n",
    "    body_text = soup.get_text(separator='\\n')\n",
    "    lines = [line.strip() for line in body_text.splitlines() if line.strip()]\n",
    "\n",
    "    # Keywords/phrases to exclude\n",
    "    unwanted_keywords = [\n",
    "        \"Complaint Forms\", \"Electronic Complaint Form Learn how to file\", \"How OCR Evaluates Complaints\",\n",
    "        \"FAQs on the Complaint Process\", \"Customer Service Standards for the Case Resolution Process\",\n",
    "        \"Complainant and Interviewee Rights and Protections\", \"Rights and protections\",\n",
    "        \"Office of Communications and Outreach\", \"Page Last Reviewed\"\n",
    "    ]\n",
    "\n",
    "    # Remove lines matching unwanted sections\n",
    "    filtered_lines = [\n",
    "        line for line in lines\n",
    "        if not any(keyword.lower() in line.lower() for keyword in unwanted_keywords)\n",
    "    ]\n",
    "\n",
    "    # Try to add Electronic Complaint Form and Fillable PDF Complaint Form links\n",
    "    extra_links_text = \"\"\n",
    "    electronic_form = soup.find('a', string=lambda text: text and 'Electronic Complaint Form' in text)\n",
    "    pdf_form = soup.find('a', string=lambda text: text and 'Fillable PDF Complaint Form' in text)\n",
    "\n",
    "    if electronic_form:\n",
    "        href = electronic_form.get('href')\n",
    "        extra_links_text += f\"\\nElectronic Complaint Form: {href}\"\n",
    "    if pdf_form:\n",
    "        href = pdf_form.get('href')\n",
    "        extra_links_text += f\"\\nFillable PDF Complaint Form: {href}\"\n",
    "\n",
    "    # Final value\n",
    "    value = ' '.join(filtered_lines) + extra_links_text\n",
    "\n",
    "    # Result dict\n",
    "    result = {key: value}\n",
    "\n",
    "    _append_to_json(result, output_filename)\n",
    "\n",
    "def _append_to_json(new_data, output_filename):\n",
    "    \"\"\"\n",
    "    Appends a dictionary of data to an existing JSON file or creates a new one.\n",
    "\n",
    "    Args:\n",
    "        new_data (dict): The dictionary data to append.\n",
    "        output_filename (str): The name of the JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_filename, 'r+', encoding='utf-8') as f:\n",
    "            try:\n",
    "                existing_data = json.load(f)\n",
    "                existing_data.update(new_data)\n",
    "                f.seek(0)\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "                f.truncate() # Remove remaining part if new data is shorter\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding existing JSON file. Overwriting with new data.\")\n",
    "                f.seek(0)\n",
    "                json.dump(new_data, f, ensure_ascii=False, indent=4)\n",
    "                f.truncate()\n",
    "    except FileNotFoundError:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(new_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to data//tab_data.json\n"
     ]
    }
   ],
   "source": [
    "ferpa_url = \"https://studentprivacy.ed.gov/ferpa\"\n",
    "civil_rights_url = \"https://www.ed.gov/laws-and-policy/civil-rights-laws\"\n",
    "file_complaint_url = \"https://www.ed.gov/laws-and-policy/civil-rights-laws/file-complaint\"\n",
    "\n",
    "append_ferpa_data(ferpa_url)\n",
    "append_civil_rights_data(civil_rights_url)\n",
    "append_file_complaint_data(file_complaint_url)\n",
    "\n",
    "print(\"Data appended to data//tab_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS index and metadata saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "with open('data/tab_data.json', 'r', encoding='utf-8') as f:\n",
    "    tab_data = json.load(f)\n",
    "\n",
    "# Combine tab title and content into a document\n",
    "documents = [f\"{key}: {value}\" for key, value in tab_data.items()]\n",
    "metadata = list(tab_data.keys())\n",
    "\n",
    "# Load a pre-trained embedding model from Hugging Face\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Create FAISS index\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # Using L2 similarity\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, 'data/faiss_index.idx')\n",
    "\n",
    "# Save the metadata for reverse lookup\n",
    "with open('data/faiss_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… FAISS index and metadata saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top 2 Results for: 'Want to know about the academic appeals.'\n",
      "\n",
      "1. Title: Grades\n",
      "   Score: 0.9125\n",
      "   Content:\n",
      "General Grade Appeal\n",
      "Grades are determined solely by the individual faculty who taught the course for the session(s) or the semester(s). A student who wishes to contest a grade must first attempt to resolve the matter with the course faculty.\n",
      "If the matter cannot be resolved with the instructor, the...\n",
      "\n",
      "2. Title: Academics\n",
      "   Score: 1.0904\n",
      "   Content:\n",
      "Academic Appeals\n",
      "Students placed on academic probation or suspension may appeal to the Academic Standards Committee by filing an appeal form with the Office of the Registrar. The student has the right to appeal any action affecting their academic status by obtaining the appropriate form from the Off...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved FAISS index and metadata\n",
    "index = faiss.read_index('data/faiss_index.idx')\n",
    "with open('data/faiss_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Also load the tab_data to fetch full content for results\n",
    "with open('data/tab_data.json', 'r', encoding='utf-8') as f:\n",
    "    tab_data = json.load(f)\n",
    "\n",
    "def search_query(user_query, top_k=2):\n",
    "    # Convert query to embedding\n",
    "    query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "\n",
    "    # Perform similarity search\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    # Print top-k matches\n",
    "    print(f\"\\nðŸ” Top {top_k} Results for: '{user_query}'\\n\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        title = metadata[idx]\n",
    "        content = tab_data[title]\n",
    "        score = distances[0][i]\n",
    "        print(f\"{i+1}. Title: {title}\")\n",
    "        print(f\"   Score: {score:.4f}\")\n",
    "        print(f\"   Content:\\n{content[:300]}{'...' if len(content) > 300 else ''}\\n\")\n",
    "\n",
    "# Example\n",
    "search_query(\"Want to know about the academic appeals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "        model=\"Llama3-8b-8192\",\n",
    "        temperature=0,\n",
    "        max_tokens=4192,\n",
    "        timeout=30,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "def generate_answer(user_query, retrieved_titles, tab_data):\n",
    "    # Combine relevant tab content\n",
    "    retrieved_docs = \"\\n\\n\".join([f\"{title}: {tab_data[title]}\" for title in retrieved_titles if title in tab_data])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant. Answer the user's question **only** based on the information provided below. \n",
    "Do **not** use any external knowledge or generate content beyond the provided documents.\n",
    "\n",
    "If the answer is **not present** in the information, reply with: \n",
    "\"I'm sorry, but that question is outside the scope of the provided information.\"\n",
    "\n",
    "Information:\n",
    "{retrieved_docs}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer in a clear, helpful, and concise manner.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Generate response using ChatGroq (LLaMA3)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Answer:\n",
      "I'm sorry, but that question is outside the scope of the provided information.\n"
     ]
    }
   ],
   "source": [
    "def search_and_generate(user_query, top_k=3):\n",
    "    query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    retrieved_titles = [metadata[idx] for idx in indices[0]]\n",
    "\n",
    "    # Generate the answer using LLaMA (ChatGroq)\n",
    "    answer = generate_answer(user_query, retrieved_titles, tab_data)\n",
    "\n",
    "    print(f\"\\nðŸ§  Answer:\\n{answer.content}\")\n",
    "\n",
    "\n",
    "# question 1 :- Want to know about the academic appeals.\n",
    "# question 2 :- How to File A Complaint for civil rights.\n",
    "# question 3 :- Can you tell me the capital of france?\n",
    "search_and_generate(\"Can you tell me the capital of france?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
