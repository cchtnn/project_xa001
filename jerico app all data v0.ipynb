{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\chtn\\gen_ai\\hitesh\\zenza_chatbot\\project_xa001\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from urllib.parse import urljoin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_website(url):\n",
    "    # Get response from the server\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 500:\n",
    "        print(\"Server error\")\n",
    "        return\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 1: Find all tab titles\n",
    "    tab_titles = soup.find_all(\"div\", class_=\"elementor-tab-title\")\n",
    "\n",
    "    # Step 2: Find all corresponding tab contents\n",
    "    tab_data = {}\n",
    "\n",
    "    for title_div in tab_titles:\n",
    "        tab_id = title_div.get(\"data-tab\")\n",
    "        tab_title = title_div.get_text(strip=True)\n",
    "\n",
    "        matching_content = soup.find(\"div\", class_=\"elementor-tab-content\", attrs={\"data-tab\": tab_id})\n",
    "        tab_content = matching_content.get_text(separator=\"\\n\", strip=True) if matching_content else \"\"\n",
    "\n",
    "        tab_data[tab_title] = tab_content\n",
    "\n",
    "    # ✅ Create 'data' folder if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    # ✅ Save as JSON file\n",
    "    with open(\"data/tab_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tab_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Data saved to data/tab_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data_from_website(\"https://www.dinecollege.edu/academics/academic-policies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ferpa_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from a FERPA-related website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    data = {}\n",
    "    h3_tags = soup.find_all('h3')\n",
    "    for h3_tag in h3_tags:\n",
    "        question = h3_tag.text.strip()\n",
    "        answer_parts = []\n",
    "        sibling = h3_tag.find_next_sibling()\n",
    "        while sibling and sibling.name in ['p', 'div', 'ul', 'ol']:\n",
    "            answer_parts.append(sibling.text.strip())\n",
    "            sibling = sibling.find_next_sibling()\n",
    "        answer = \" \".join(answer_parts).strip()\n",
    "        if question and answer:\n",
    "            data[question] = answer\n",
    "\n",
    "    modified_data = {}\n",
    "    for key, value in data.items():\n",
    "        new_key = key.rstrip(\":\")\n",
    "        modified_value = value.replace(\"Back to Top\", \"\").strip()\n",
    "        if len(modified_value.split()) >= 5:\n",
    "            modified_data[new_key] = modified_value\n",
    "\n",
    "    _append_to_json(modified_data, output_filename)\n",
    "\n",
    "def append_civil_rights_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from a civil rights laws website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    final_data = {}\n",
    "\n",
    "    # Extract the main heading and description\n",
    "    main_heading = soup.find('h1', class_='usa-hero__heading')\n",
    "    main_desc = soup.find('div', class_='field--name-body')\n",
    "\n",
    "    if main_heading and main_desc:\n",
    "        title = main_heading.text.strip()\n",
    "        description = main_desc.text.strip()\n",
    "        final_data[title] = description\n",
    "\n",
    "    # Now extract the cards information\n",
    "    cards = soup.find_all('div', class_='card-image-top-txt')\n",
    "\n",
    "    for card in cards:\n",
    "        card_title_tag = card.find('div', class_='field--name-field-ed-card-image-top-title')\n",
    "        card_summary_tag = card.find('div', class_='field--name-field-ed-card-image-top-summary')\n",
    "        card_link_tag = card.find('div', class_='field--name-field-ed-card-image-top-link')\n",
    "\n",
    "        if card_title_tag and card_summary_tag:\n",
    "            card_title = card_title_tag.text.strip()\n",
    "            card_summary = card_summary_tag.text.strip()\n",
    "\n",
    "            # Get the link if available\n",
    "            link = \"\"\n",
    "            if card_link_tag and card_link_tag.find('a'):\n",
    "                href = card_link_tag.find('a')['href']\n",
    "                if href.startswith(\"/\"):\n",
    "                    href = \"https://www.ed.gov\" + href\n",
    "                link = href\n",
    "            final_data[card_title] = f\"{card_summary} link :- {link}\".strip()\n",
    "\n",
    "    _append_to_json(final_data, output_filename)\n",
    "\n",
    "def append_file_complaint_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    \"\"\"\n",
    "    Fetches data from the file a complaint website, processes it, and appends it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        output_filename (str, optional): The name of the JSON file to save/append data to.\n",
    "            Defaults to \"data//tab_data.json\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Remove unnecessary tags\n",
    "    for tag in soup([\"script\", \"style\", \"footer\", \"nav\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove known banners or sections\n",
    "    for div in soup.find_all(['div', 'section'], class_=[\n",
    "        'usa-banner', 'header', 'navigation', 'menu', 'site-header',\n",
    "        'usa-footer', 'main-header', 'branding', 'footer-links'\n",
    "    ]):\n",
    "        div.decompose()\n",
    "\n",
    "    for elem in soup.find_all(id=[\n",
    "        'header', 'footer', 'navbar', 'skip-link', 'back-to-top'\n",
    "    ]):\n",
    "        elem.decompose()\n",
    "\n",
    "    # Get heading\n",
    "    heading = soup.find('h1')\n",
    "    key = heading.get_text(strip=True) if heading else \"No Heading Found\"\n",
    "\n",
    "    # Collect all visible text\n",
    "    body_text = soup.get_text(separator='\\n')\n",
    "    lines = [line.strip() for line in body_text.splitlines() if line.strip()]\n",
    "\n",
    "    # Keywords/phrases to exclude\n",
    "    unwanted_keywords = [\n",
    "        \"Complaint Forms\", \"Electronic Complaint Form Learn how to file\", \"How OCR Evaluates Complaints\",\n",
    "        \"FAQs on the Complaint Process\", \"Customer Service Standards for the Case Resolution Process\",\n",
    "        \"Complainant and Interviewee Rights and Protections\", \"Rights and protections\",\n",
    "        \"Office of Communications and Outreach\", \"Page Last Reviewed\"\n",
    "    ]\n",
    "\n",
    "    # Remove lines matching unwanted sections\n",
    "    filtered_lines = [\n",
    "        line for line in lines\n",
    "        if not any(keyword.lower() in line.lower() for keyword in unwanted_keywords)\n",
    "    ]\n",
    "\n",
    "    # Try to add Electronic Complaint Form and Fillable PDF Complaint Form links\n",
    "    extra_links_text = \"\"\n",
    "    electronic_form = soup.find('a', string=lambda text: text and 'Electronic Complaint Form' in text)\n",
    "    pdf_form = soup.find('a', string=lambda text: text and 'Fillable PDF Complaint Form' in text)\n",
    "\n",
    "    if electronic_form:\n",
    "        href = electronic_form.get('href')\n",
    "        extra_links_text += f\"\\nElectronic Complaint Form: {href}\"\n",
    "    if pdf_form:\n",
    "        href = pdf_form.get('href')\n",
    "        extra_links_text += f\"\\nFillable PDF Complaint Form: {href}\"\n",
    "\n",
    "    # Final value\n",
    "    value = ' '.join(filtered_lines) + extra_links_text\n",
    "\n",
    "    # Result dict\n",
    "    result = {key: value}\n",
    "\n",
    "    _append_to_json(result, output_filename)\n",
    "\n",
    "def _append_to_json(new_data, output_filename):\n",
    "    \"\"\"\n",
    "    Appends a dictionary of data to an existing JSON file or creates a new one.\n",
    "\n",
    "    Args:\n",
    "        new_data (dict): The dictionary data to append.\n",
    "        output_filename (str): The name of the JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_filename, 'r+', encoding='utf-8') as f:\n",
    "            try:\n",
    "                existing_data = json.load(f)\n",
    "                existing_data.update(new_data)\n",
    "                f.seek(0)\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "                f.truncate() # Remove remaining part if new data is shorter\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding existing JSON file. Overwriting with new data.\")\n",
    "                f.seek(0)\n",
    "                json.dump(new_data, f, ensure_ascii=False, indent=4)\n",
    "                f.truncate()\n",
    "    except FileNotFoundError:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(new_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to data//tab_data.json\n"
     ]
    }
   ],
   "source": [
    "ferpa_url = \"https://studentprivacy.ed.gov/ferpa\"\n",
    "civil_rights_url = \"https://www.ed.gov/laws-and-policy/civil-rights-laws\"\n",
    "file_complaint_url = \"https://www.ed.gov/laws-and-policy/civil-rights-laws/file-complaint\"\n",
    "\n",
    "# append_ferpa_data(ferpa_url)\n",
    "# append_civil_rights_data(civil_rights_url)\n",
    "# append_file_complaint_data(file_complaint_url)\n",
    "\n",
    "print(\"Data appended to data//tab_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_as_text(table):\n",
    "    \"\"\"Convert HTML table to a formatted string.\"\"\"\n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        rows.append('\\t'.join(cols))\n",
    "    return '\\n'.join(rows)\n",
    "\n",
    "def append_fafsa_data(url, output_filename=\"data//tab_data.json\"):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # print(\"Soup :-\", soup)\n",
    "    container = soup.find('div', class_='field field--name-body field--type-text-with-summary field--label-hidden field__item')\n",
    "    if not container:\n",
    "        print(\"No content container found.\")\n",
    "        return\n",
    "\n",
    "    result = defaultdict(str)\n",
    "    current_header = None\n",
    "\n",
    "    # Iterate through direct children of the container\n",
    "    for tag in container.find_all(recursive=False):\n",
    "        if tag.name and tag.name.startswith('h'):\n",
    "            current_header = tag.get_text(strip=True)\n",
    "            result[current_header] = ''\n",
    "\n",
    "        elif tag.name == 'p' and current_header:\n",
    "            paragraph_text = tag.get_text(strip=True)\n",
    "            if paragraph_text:\n",
    "                result[current_header] += paragraph_text + '\\n'\n",
    "\n",
    "        elif tag.name == 'table' and current_header:\n",
    "            table_text = extract_table_as_text(tag)\n",
    "            if table_text:\n",
    "                result[current_header] += '\\n' + table_text + '\\n'\n",
    "\n",
    "        elif tag.name == 'div' and current_header:\n",
    "            # Look for any nested tables inside divs (e.g. grid or layout blocks)\n",
    "            nested_table = tag.find('table')\n",
    "            if nested_table:\n",
    "                table_text = extract_table_as_text(nested_table)\n",
    "                if table_text:\n",
    "                    result[current_header] += '\\n' + table_text + '\\n'\n",
    "\n",
    "    # Strip trailing whitespace\n",
    "    result = {k: v.strip() for k, v in result.items() if len(v.strip())>1}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under developement\n",
    "# append_fafsa_data(\"https://www.ed.gov/higher-education/paying-college/better-fafsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial correct. will delete that later.\n",
    "\n",
    "# def extract_table_as_text(table):\n",
    "#     \"\"\"Convert HTML table to a formatted string.\"\"\"\n",
    "#     rows = []\n",
    "#     for row in table.find_all('tr'):\n",
    "#         cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "#         rows.append('\\t'.join(cols))\n",
    "#     return '\\n'.join(rows)\n",
    "\n",
    "# def extract_text_with_links(element, base_url):\n",
    "#     \"\"\"Extract text from element and include absolute links.\"\"\"\n",
    "#     # Initialize output\n",
    "#     result = \"\"\n",
    "    \n",
    "#     # Process all children\n",
    "#     for child in element.children:\n",
    "#         if child.name == 'a':\n",
    "#             # Extract link info\n",
    "#             link_text = child.get_text(strip=True)\n",
    "#             link_url = child.get('href', '')\n",
    "            \n",
    "#             # Convert to absolute URL if it's relative\n",
    "#             if link_url and not link_url.startswith(('http://', 'https://')):\n",
    "#                 link_url = urljoin(base_url, link_url)\n",
    "            \n",
    "#             # Add formatted link: text [url]\n",
    "#             if link_url:\n",
    "#                 result += f\"{link_text} [{link_url}]\"\n",
    "#             else:\n",
    "#                 result += link_text\n",
    "#         elif isinstance(child, str):\n",
    "#             # Add plain text\n",
    "#             result += child\n",
    "#         elif child.name:  # Check if it's an element\n",
    "#             # Recursively process other elements\n",
    "#             result += extract_text_with_links(child, base_url)\n",
    "    \n",
    "#     return result.strip()\n",
    "\n",
    "# def extract_list_content(heading_element, base_url):\n",
    "#     \"\"\"Extract list content with absolute links that follows a heading element.\"\"\"\n",
    "#     content = []\n",
    "#     current = heading_element.next_sibling\n",
    "    \n",
    "#     while current:\n",
    "#         if current.name == 'ul':\n",
    "#             for li in current.find_all('li', recursive=True):\n",
    "#                 # Extract text with links for each list item\n",
    "#                 item_content = extract_text_with_links(li, base_url)\n",
    "#                 content.append(item_content)\n",
    "#             break\n",
    "#         elif current.name in ['h2', 'h3']:  # Stop if we hit another heading\n",
    "#             break\n",
    "#         current = current.next_sibling\n",
    "    \n",
    "#     return content\n",
    "\n",
    "# def append_fafsa_data(url, output_filename=\"data/tab_data.json\"):\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error fetching URL: {e}\")\n",
    "#         return\n",
    "    \n",
    "#     # Extract base URL for converting relative links to absolute\n",
    "#     base_url = url\n",
    "    \n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     container = soup.find('div', class_='field field--name-body field--type-text-with-summary field--label-hidden field__item')\n",
    "#     if not container:\n",
    "#         print(\"No content container found.\")\n",
    "#         return\n",
    "    \n",
    "#     # Dictionary to store all headers and their associated list items\n",
    "#     result = {}\n",
    "    \n",
    "#     # Find all header tags (h2, h3)\n",
    "#     headers = container.find_all(['h2', 'h3'])\n",
    "    \n",
    "#     for header in headers:\n",
    "#         header_text = header.get_text(strip=True)\n",
    "#         list_items = extract_list_content(header, base_url)\n",
    "        \n",
    "#         # Only add headers that have list items\n",
    "#         if list_items:\n",
    "#             result[header_text] = list_items\n",
    "    \n",
    "#     # Ensure directory exists\n",
    "#     os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    \n",
    "#     # Save to file\n",
    "#     with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(result, f , ensure_ascii=False , indent=2)\n",
    "    \n",
    "#     print(f\"Data saved to {output_filename}\")\n",
    "#     return result\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     url = \"https://www.ed.gov/higher-education/paying-college/better-fafsa\"\n",
    "#     result = append_fafsa_data(url)\n",
    "#     print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_table_as_text(table):\n",
    "#     rows = []\n",
    "#     for row in table.find_all('tr'):\n",
    "#         cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "#         rows.append('\\t'.join(cols))\n",
    "#     return '\\n'.join(rows)\n",
    "\n",
    "# def extract_text_with_links(element, base_url):\n",
    "#     result = \"\"\n",
    "#     for child in element.children:\n",
    "#         if child.name == 'a':\n",
    "#             link_text = child.get_text(strip=True)\n",
    "#             link_url = child.get('href', '')\n",
    "#             if link_url and not link_url.startswith(('http://', 'https://')):\n",
    "#                 link_url = urljoin(base_url, link_url)\n",
    "#             result += f\"{link_text} [{link_url}]\" if link_url else link_text\n",
    "#         elif isinstance(child, str):\n",
    "#             result += child\n",
    "#         elif child.name:\n",
    "#             result += extract_text_with_links(child, base_url)\n",
    "#     return result.strip()\n",
    "\n",
    "# def extract_list_content(heading_element, base_url):\n",
    "#     content = []\n",
    "#     current = heading_element.next_sibling\n",
    "\n",
    "#     while current:\n",
    "#         if isinstance(current, NavigableString):\n",
    "#             current = current.next_sibling\n",
    "#             continue\n",
    "#         if current.name == 'ul':\n",
    "#             for li in current.find_all('li', recursive=True):\n",
    "#                 item_content = extract_text_with_links(li, base_url)\n",
    "#                 content.append(item_content)\n",
    "#             break\n",
    "#         elif current.name in ['h2', 'h3']:\n",
    "#             break\n",
    "#         current = current.next_sibling\n",
    "\n",
    "#     return content\n",
    "\n",
    "# def extract_h3_with_paragraphs(soup):\n",
    "#     \"\"\"Extract <h3> inside .panel-heading and their <p> inside .panel-body.\"\"\"\n",
    "#     result = {}\n",
    "#     panels = soup.find_all(\"div\", class_=\"panel panel-primary\")\n",
    "#     for panel in panels:\n",
    "#         heading = panel.find(\"div\", class_=\"panel-heading\")\n",
    "#         body = panel.find(\"div\", class_=\"panel-body\")\n",
    "#         if heading and body:\n",
    "#             h3 = heading.find(\"h3\")\n",
    "#             p = body.find(\"p\")\n",
    "#             if h3 and p:\n",
    "#                 heading_text = h3.get_text(strip=True)\n",
    "#                 paragraph_text = p.get_text(strip=True)\n",
    "#                 result[heading_text] = paragraph_text\n",
    "#     return result\n",
    "\n",
    "# def append_fafsa_data(url, output_filename=\"data/tab_data.json\"):\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error fetching URL: {e}\")\n",
    "#         return\n",
    "\n",
    "#     base_url = url\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     container = soup.find('div', class_='field field--name-body field--type-text-with-summary field--label-hidden field__item')\n",
    "#     if not container:\n",
    "#         print(\"No content container found.\")\n",
    "#         return\n",
    "\n",
    "#     result = {}\n",
    "\n",
    "#     headers = container.find_all(['h2', 'h3'])\n",
    "#     for header in headers:\n",
    "#         header_text = header.get_text(strip=True)\n",
    "#         list_items = extract_list_content(header, base_url)\n",
    "#         if list_items:\n",
    "#             result[header_text] = list_items\n",
    "\n",
    "#     # Extract from panel structure\n",
    "#     panel_data = extract_h3_with_paragraphs(soup)  # <- now uses full soup\n",
    "#     for heading, paragraph in panel_data.items():\n",
    "#         if heading not in result:\n",
    "#             result[heading] = paragraph\n",
    "\n",
    "#     os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "\n",
    "#     with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(result, f, indent=2)\n",
    "\n",
    "#     print(f\"Data saved to {output_filename}\")\n",
    "#     return result\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     url = \"https://www.ed.gov/higher-education/paying-college/better-fafsa\"\n",
    "#     result = append_fafsa_data(url)\n",
    "#     print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_as_text(table):\n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        rows.append('\\t'.join(cols))\n",
    "    return '\\n'.join(rows)\n",
    "\n",
    "def extract_text_with_links(element, base_url):\n",
    "    result = \"\"\n",
    "    for child in element.children:\n",
    "        if child.name == 'a':\n",
    "            link_text = child.get_text(strip=True)\n",
    "            link_url = child.get('href', '')\n",
    "            if link_url and not link_url.startswith(('http://', 'https://')):\n",
    "                link_url = urljoin(base_url, link_url)\n",
    "            result += f\"{link_text} [{link_url}]\" if link_url else link_text\n",
    "        elif isinstance(child, str):\n",
    "            result += child\n",
    "        elif child.name:\n",
    "            result += extract_text_with_links(child, base_url)\n",
    "    return result.strip()\n",
    "\n",
    "def extract_list_content(heading_element, base_url):\n",
    "    content = []\n",
    "    current = heading_element.next_sibling\n",
    "\n",
    "    while current:\n",
    "        if isinstance(current, NavigableString):\n",
    "            current = current.next_sibling\n",
    "            continue\n",
    "        if current.name == 'ul':\n",
    "            for li in current.find_all('li', recursive=True):\n",
    "                item_content = extract_text_with_links(li, base_url)\n",
    "                content.append(item_content)\n",
    "            break\n",
    "        elif current.name in ['h2', 'h3']:\n",
    "            break\n",
    "        current = current.next_sibling\n",
    "\n",
    "    return content\n",
    "\n",
    "def extract_h3_with_paragraphs(soup):\n",
    "    result = {}\n",
    "    panels = soup.find_all(\"div\", class_=\"panel panel-primary\")\n",
    "    for panel in panels:\n",
    "        heading = panel.find(\"div\", class_=\"panel-heading\")\n",
    "        body = panel.find(\"div\", class_=\"panel-body\")\n",
    "        if heading and body:\n",
    "            h3 = heading.find(\"h3\")\n",
    "            p = body.find(\"p\")\n",
    "            if h3 and p:\n",
    "                heading_text = h3.get_text(strip=True)\n",
    "                paragraph_text = p.get_text(strip=True)\n",
    "                result[heading_text] = paragraph_text\n",
    "    return result\n",
    "\n",
    "def append_fafsa_data(url, output_filename=\"data/tab_data.json\"):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return\n",
    "\n",
    "    base_url = url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    container = soup.find('div', class_='field field--name-body field--type-text-with-summary field--label-hidden field__item')\n",
    "    if not container:\n",
    "        print(\"No content container found.\")\n",
    "        return\n",
    "\n",
    "    result = defaultdict(str)\n",
    "    current_header = None\n",
    "\n",
    "    for tag in container.find_all(recursive=False):\n",
    "        if tag.name and tag.name.startswith('h'):\n",
    "            current_header = tag.get_text(strip=True)\n",
    "            result[current_header] = ''\n",
    "\n",
    "        elif tag.name == 'p' and current_header:\n",
    "            paragraph_text = tag.get_text(strip=True)\n",
    "            if paragraph_text:\n",
    "                result[current_header] += paragraph_text + '\\n'\n",
    "\n",
    "        elif tag.name == 'table' and current_header:\n",
    "            table_text = extract_table_as_text(tag)\n",
    "            if table_text:\n",
    "                result[current_header] += '\\n' + table_text + '\\n'\n",
    "\n",
    "        elif tag.name == 'div' and current_header:\n",
    "            nested_table = tag.find('table')\n",
    "            if nested_table:\n",
    "                table_text = extract_table_as_text(nested_table)\n",
    "                if table_text:\n",
    "                    result[current_header] += '\\n' + table_text + '\\n'\n",
    "\n",
    "    result = {k: v.strip() for k, v in result.items() if len(v.strip()) > 1}\n",
    "\n",
    "    headers = container.find_all(['h2', 'h3'])\n",
    "    for header in headers:\n",
    "        header_text = header.get_text(strip=True)\n",
    "        list_items = extract_list_content(header, base_url)\n",
    "        if list_items:\n",
    "            if header_text in result:\n",
    "                result[header_text] += '\\n' + '\\n'.join(list_items)\n",
    "            else:\n",
    "                result[header_text] = '\\n'.join(list_items)\n",
    "\n",
    "    panel_data = extract_h3_with_paragraphs(soup)\n",
    "    for heading, paragraph in panel_data.items():\n",
    "        if heading in result:\n",
    "            result[heading] += '\\n' + paragraph\n",
    "        else:\n",
    "            result[heading] = paragraph\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    _append_to_json(result, output_filename)\n",
    "    print(f\"Data appended to {output_filename}\")\n",
    "    return result\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     url = \"https://www.ed.gov/higher-education/paying-college/better-fafsa\"\n",
    "#     result = append_fafsa_data(url)\n",
    "#     print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index and metadata saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "with open('data/tab_data.json', 'r', encoding='utf-8') as f:\n",
    "    tab_data = json.load(f)\n",
    "\n",
    "# Combine tab title and content into a document\n",
    "documents = [f\"{key}: {value}\" for key, value in tab_data.items()]\n",
    "metadata = list(tab_data.keys())\n",
    "\n",
    "# Load a pre-trained embedding model from Hugging Face\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Create FAISS index\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # Using L2 similarity\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, 'data/faiss_index.idx')\n",
    "\n",
    "# Save the metadata for reverse lookup\n",
    "with open('data/faiss_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ FAISS index and metadata saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved FAISS index and metadata\n",
    "index = faiss.read_index('data/faiss_index.idx')\n",
    "with open('data/faiss_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Also load the tab_data to fetch full content for results\n",
    "with open('data/tab_data.json', 'r', encoding='utf-8') as f:\n",
    "    tab_data = json.load(f)\n",
    "\n",
    "# def search_query(user_query, top_k=2):\n",
    "#     # Convert query to embedding\n",
    "#     query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "\n",
    "#     # Perform similarity search\n",
    "#     distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "#     # Print top-k matches\n",
    "#     print(f\"\\n🔍 Top {top_k} Results for: '{user_query}'\\n\")\n",
    "#     for i, idx in enumerate(indices[0]):\n",
    "#         title = metadata[idx]\n",
    "#         content = tab_data[title]\n",
    "#         score = distances[0][i]\n",
    "#         print(f\"{i+1}. Title: {title}\")\n",
    "#         print(f\"   Score: {score:.4f}\")\n",
    "#         print(f\"   Content:\\n{content[:300]}{'...' if len(content) > 300 else ''}\\n\")\n",
    "\n",
    "# # Example\n",
    "# search_query(\"Want to know about the academic appeals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "# model\n",
    "# tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Grades', 'Academics'],\n",
       " ['General Grade Appeal\\nGrades are determined solely by the individual faculty who taught the course for the session(s) or the semester(s). A student who wishes to contest a grade must first attempt to resolve the matter with the course faculty.\\nIf the matter cannot be resolved with the instructor, the student may appeal to the appropriate Dean of School. The student must provide the evidence as to why the grade posted by the faculty is an error. if the matter is not resolved with the Dean of School, the student may appeal a final time to the Academic Standards Committee. The decision of the Academic Standards committee is final.\\nGrades may be appealed within one academic year. The Grade Appeal Form can be obtained by Office of the Registrar and will guide students through each of the three steps.\\nGrade Point Average (GPA) System\\nGrade Point Average (GPA) refers to the average grade at any particular time during, or at the end of, any particular semester. It is determined by the total number of Quality Points Earned (QPE) divided by total number of credits.\\nThe Cumulative Grade Point Average (CGPA) is the average for all courses taken at the college. It is computed by taking the sum of all Quality Points Earned (QPE) while in attendance at Diné College divided by the sum of all eligible credit hours earned.\\nQuality points are figured as follows: A = 4 quality points, B = 3 quality points, C = 2 quality points, D = 1 quality point, and F = 0. For instance, if a student receives an A in ENG 101 (3 credits x 4 quality points = 12 QPE), B in MTH 106 (3 credits x 3 quality points = 9 QPE), and C in PSY 111 (3 credits x 2 quality points = 6 QPE), that person’s GPA would be 3.00 (27 total quality points divided by 9 credits = 3.00).\\nThe following rules are considered in computing the CGPA:\\nCourses that are repeated are counted only once for credit and the CGPA; however, all repeated courses appear on the student’s transcript.\\nThe higher grade is used for computing the CGPA whenever a course is repeated.\\nCertain courses can be repeated for credit and therefore are exempt from the two rules above (e.g., 099, 199, and 299 courses; students should consult with their advisor).\\nWhen grades of “I” or “IP” have been changed to letter grades, they are computed in the CGPA.\\nA “CR” grade is computed in the total credit hours earned, but is not included in the CGPA.\\nIndividual instructors or academic divisions have specific policies and requirements for the “IP” grade. Students are advised to check each course syllabus carefully to be informed about the “IP” grade policy for courses.\\nF, a failing grade, is shown on the transcript and computed in the CGPA unless the student retakes the course and passes, at which time the passing grade is entered on the transcript and the quality points are computed in the CGPA and the semester GPA.\\nMidterm and Final Grade Report\\nThe midterm examinations occur during the 8th week of classes during the Fall and Spring semesters. Midterm grades are not entered on the permanent record. Final examinations are scheduled at the end of the semester and must be taken during scheduled times.\\nRepeating Courses\\nStudents may repeat courses previously taken at the College to better their understanding or to improve their grades. A transcript shows that the course was repeated, but only the higher grade is used to compute the student’s CGPA). Repeating or retaking a class can affect a student’s financial aid. (Please see repeat or retake in Financial Aid Policies.',\n",
       "  'Academic Appeals\\nStudents placed on academic probation or suspension may appeal to the Academic Standards Committee by filing an appeal form with the Office of the Registrar. The student has the right to appeal any action affecting their academic status by obtaining the appropriate form from the Office of the Registrar: Appeal of Suspension, Appeal of Probation, Grade Appeal, or General Appeal.\\nAcademic Integrity\\nStudents are responsible for the integrity of their academic work. Examples of academic dishonesty include but are not limited to, obtaining unauthorized assistance in any academic work; cheating on a test; plagiarism; quoting without proper credit; modifying any examination, paper, record, report or project without the instructor’s approval for obtaining additional credit or an improved grade; and, representing the work of others as one’s own. Some of the penalties that may be imposed include: warning (written or oral); reducing the grade for the assignment, test, or project; reducing the grade for the course; assigning a failing grade for the course; dismissing the student from the course and issuing a grade of “W”; academic probation or suspension; expulsion; and recording the decision in the student’s academic record.\\nAcademic Probation\\nStudents who do not maintain a semester GPA of 2.00 are subject to academic probation for the next semester of regular attendance. Academic probation is not recorded on the transcript. The criteria for full- or part-time students are:\\nFull-time students, upon completion of 12 or more semester credit hours, are placed on academic probation if they have a GPA less than 2.00 (higher levels specific to each Bachelor’s program). If the number of semester credit hours drops below 12 as a result of an “In Progress” (IP) grade (see below), students are still considered to be making satisfactory progress.\\nPart-time students, upon completion of 16 or more cumulative semester credit hours, are placed on academic probation when a total of 11 or fewer credit hours of work have been attempted in a semester and cumulative GPA is less than a 2.00.\\nStudents on academic probation may take no more than 14 credit hours per semester unless approved by an advisor.\\nAcademic Recognition\\nAcademic recognition can be achieved as follow:\\nPresident’s Honor List: Students must achieve a semester GPA of 4.00, complete a minimum of 12 credit hours, and receive no grades of “I” or “IP.” “Honors” will be noted on transcript.\\nProvost’s Honor List: Students must achieve a semester GPA of 3.50 –3.99, complete a minimum of 12 credit hours, and receive no grades of “I” or “IP.” “Honors” will be noted on transcript.\\nAcademic Standing\\nStudents must maintain a 2.00 cumulative grade point average (CGPA) throughout their program of study (or higher levels in the Bachelor’s degree programs; consult the program handbooks for details). When students receive final grades of A, B, C, D, or F, they are considered credit hours attempted and earned. W, I, or IP grades earn no credit and are not considered hours completed. Transfer credit hours accepted by Diné College are not calculated in cumulative grade point averages for determining satisfactory progress.\\nAcademic Status Change\\nWhen change of an “Incomplete” (I) or “In Progress” (IP) grade alters a student’s academic status, the student is notified of the change in writing by Office of the Registrar. When the changes are formally adjusted, students are placed on good standing, academic probation or suspension and the restriction is effective immediately.\\nAcademic Suspension\\nStudents who do not satisfactorily meet the required minimum standards during the probationary semester will be placed on academic suspension. Academic suspension means the student will not be permitted to enroll in any classes at Diné College for the semester of the suspension. Upon returning, the student will be placed on academic probation until minimum standards for satisfactory progress are met.\\nAcademic suspension is not recorded on a student’s transcript. Students who are placed on academic probation or academic suspension are subject to additional regulations regarding Satisfactory Academic Progress (SAP). A student may appeal academic suspension by obtaining appeal information from the Office of the Registrar. Program-specific policies regarding academic suspension apply to each of the Bachelor’s degree programs. Consult the program handbooks for details.\\nFor more information, contact Office of the Registrar at (928) 724-6630/6631/6632\\nContact Us\\nDiné College—Office of the Registrar\\nP.O.Box C-04\\nTsaile, Arizona, 86556\\nregistrar@dinecollege.edu\\n(928) 724-6630'],\n",
       " array([[0.91251934, 1.0904188 ]], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def search_query(user_query, index, model, metadata, tab_data, top_k=2,distance_threshold=0.2):\n",
    "#     query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "#     # print(\"Query Vector :\", query_vector)\n",
    "#     distances, indices = index.search(query_vector, top_k)\n",
    "    \n",
    "#     # retrieved_titles = [metadata[idx] for idx in indices[0]]\n",
    "#     # retrieved_docs = [tab_data[title] for title in retrieved_titles]\n",
    "\n",
    "#     valid_indices = [\n",
    "#         i for i, dist in enumerate(distances[0])\n",
    "#         if dist < distance_threshold and indices[0][i] < len(metadata)\n",
    "#     ]\n",
    "\n",
    "#     retrieved_titles = [metadata[indices[0][i]] for i in valid_indices]\n",
    "#     retrieved_docs = [tab_data[title] for title in retrieved_titles if title in tab_data]\n",
    "\n",
    "#     return retrieved_titles, retrieved_docs, distances\n",
    "\n",
    "# search_query(\"Want to know about the academic appeals.\", index, model, metadata, tab_data)\n",
    "\n",
    "def search_query(user_query, index, model, metadata, tab_data, top_k=2, distance_threshold=0.5):\n",
    "    def get_matches(threshold):\n",
    "        query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "        distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "        if threshold is None:\n",
    "            valid_indices = [\n",
    "                i for i in range(len(indices[0]))\n",
    "                if indices[0][i] < len(metadata)\n",
    "            ]\n",
    "        else:\n",
    "            valid_indices = [\n",
    "                i for i, dist in enumerate(distances[0])\n",
    "                if dist < threshold and indices[0][i] < len(metadata)\n",
    "            ]\n",
    "\n",
    "        retrieved_titles = [metadata[indices[0][i]] for i in valid_indices]\n",
    "        retrieved_docs = [tab_data[title] for title in retrieved_titles if title in tab_data]\n",
    "\n",
    "        return retrieved_titles, retrieved_docs, distances\n",
    "\n",
    "    # First attempt: use threshold\n",
    "    retrieved_titles, retrieved_docs, distances = get_matches(distance_threshold)\n",
    "\n",
    "    # Fallback attempt: no threshold if nothing found\n",
    "    if not retrieved_titles or not retrieved_docs:\n",
    "        retrieved_titles, retrieved_docs, distances = get_matches(None)\n",
    "\n",
    "    return retrieved_titles, retrieved_docs, distances\n",
    "\n",
    "\n",
    "search_query(\"Want to know about the academic appeals.\", index, model, metadata, tab_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "        model=\"Llama3-8b-8192\",\n",
    "        temperature=0,\n",
    "        max_tokens=8000,\n",
    "        timeout=30,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "def generate_answer(user_query, retrieved_titles, tab_data):\n",
    "    # Combine relevant tab content\n",
    "    retrieved_docs = \"\\n\\n\".join([f\"{title}: {tab_data[title]}\" for title in retrieved_titles if title in tab_data])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert assistant helping users understand resources related to FAFSA. \n",
    "        Answer the user's question **only** based on the information provided below. \n",
    "        Do **not** use any external knowledge or make assumptions beyond the provided documents.\n",
    "\n",
    "        When answering:\n",
    "        - Use a friendly, guiding tone.\n",
    "        - Structure the answer in a **clear, easy-to-follow manner**.\n",
    "        - Use **storytelling** where appropriate to guide the user step-by-step.\n",
    "        - Highlight important tools or resources using bullet points or bold text.\n",
    "        - Group information by relevant audience (e.g., students, educators, officials) if applicable.\n",
    "\n",
    "        If the answer is **not present** in the information, reply with: \n",
    "        \"I'm sorry, but that question is outside the scope of the provided information.\"\n",
    "\n",
    "        Information:\n",
    "        {retrieved_docs}\n",
    "\n",
    "        Question: {user_query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Generate response using ChatGroq (LLaMA3)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Answer:\n",
      "I'd be happy to help you understand the resources available for students and families, high school educators and college access counselors, and college officials.\n",
      "\n",
      "**For Students and Families:**\n",
      "\n",
      "* The **FAFSA Toolkit for Students and Families** is a great resource to get started with the FAFSA process. You can find it at [https://www.ed.gov/sites/ed/files/finaid/info/apply/fafsa-toolkit-students-families.pdf].\n",
      "* Learn about the **Better FAFSA Form** and check out the slide deck at [https://www.ed.gov/sites/ed/files/finaid/info/apply/better-fafsa-slide-deck.pdf].\n",
      "* Read the **FAFSA Pro Tips** at [https://studentaid.gov/announcements-events/fafsa-support/pro-tips] to help you successfully complete the FAFSA form.\n",
      "* Watch the video on **Applying for Financial Aid with the FAFSA Form** at [https://www.youtube.com/watch?v=UupEQdS2VMY].\n",
      "* Use the **Federal Student Aid Estimator** at [https://studentaid.gov/aid-estimator/] to receive an estimate of how much federal student aid you may be eligible to receive.\n",
      "* Chat with the **Sacha chatbot** at [https://d1qaw0xov0bofv.cloudfront.net] to get answers to your FAFSA questions.\n",
      "\n",
      "**For High School Educators and College Access Counselors:**\n",
      "\n",
      "* Check out the **FAFSA Toolkit for Educators and Counselors** at [https://www.ed.gov/sites/ed/files/finaid/info/apply/fafsa-toolkit-educators-counselors.pdf].\n",
      "* Use the **Financial Aid Toolkit** at [https://financialaidtoolkit.ed.gov/tk/].\n",
      "* Read the article **5 Things College Access Professionals Should Know** at [https://financialaidtoolkit.ed.gov/tk/announcement-detail.jsp?id=5-things-college-access-professionals-should-know].\n",
      "* Watch the videos on the **Better FAFSA** at [https://www.youtube.com/watch?v=0D8ytYCTeSY&list=PL1tCTXATxf-qJha84KeHTZ2-8d7hTf2Up&index=5].\n",
      "* Bookmark the page [https://fsapartners.ed.gov/knowledge-center/topics/fafsa-simplification-information/2024-25-fafsa-updates] for regular status updates and resources related to the better FAFSA form.\n",
      "* Sign up for updates on webinars and other information at [https://fsapartners.ed.gov/subscriptions/].\n",
      "\n",
      "**For College Officials:**\n",
      "\n",
      "* Check out the **FAFSA Toolkit for Colleges and Universities** at [https://www.ed.gov/sites/ed/files/finaid/info/apply/fafsa-toollkit-colleges-universities.pdf].\n",
      "* Use the **FSA Knowledge Center** at [https://fsapartners.ed.gov/knowledge-center/topics/fafsa-simplification-information] for all information, guidance, and training related to the better FAFSA.\n",
      "* Watch the videos on the **Better FAFSA** at [https://www.youtube.com/watch?v=0D8ytYCTeSY&list=PL1tCTXATxf-qJha84KeHTZ2-8d7hTf2Up&index=5].\n",
      "* Bookmark the page [https://fsapartners.ed.gov/knowledge-center/topics/fafsa-simplification-information/2024-25-fafsa-updates] for regular status updates and resources related to the better FAFSA form.\n",
      "* Sign up for updates on webinars and other information at [https://fsapartners.ed.gov/subscriptions/].\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "def search_and_generate(user_query, top_k=3):\n",
    "    query_vector = model.encode([user_query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    retrieved_titles = [metadata[idx] for idx in indices[0]]\n",
    "\n",
    "    # Generate the answer using LLaMA (ChatGroq)\n",
    "    answer = generate_answer(user_query, retrieved_titles, tab_data)\n",
    "\n",
    "    print(f\"\\n🧠 Answer:\\n{answer.content}\")\n",
    "\n",
    "\n",
    "# question 1 :- Want to know about the academic appeals.\n",
    "# question 2 :- How to File A Complaint for civil rights.\n",
    "# question 3 :- Can you tell me the capital of france?\n",
    "# question 4 :- Want to understand about the Resources for students and families, high school educators and college access counselors, and college officials\n",
    "\n",
    "search_and_generate(\"Want to understand about the Resources for students and families, high school educators and college access counselors, and college officials\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
